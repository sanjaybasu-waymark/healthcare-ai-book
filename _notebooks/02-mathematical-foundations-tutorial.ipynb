{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare AI Implementation Guide - Chapter 2\n",
    "## Mathematical Foundations for Healthcare AI\n",
    "\n",
    "This notebook provides comprehensive implementations of the mathematical concepts essential for healthcare AI applications.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand probability theory applications in clinical decision making\n",
    "- Implement Bayesian inference for diagnostic reasoning\n",
    "- Apply information theory to healthcare data analysis\n",
    "- Master optimization techniques for clinical machine learning\n",
    "- Implement causal inference methods for healthcare research\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic knowledge of calculus and linear algebra\n",
    "- Python programming experience\n",
    "- Understanding of clinical concepts from Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Healthcare AI Mathematical Foundations - Interactive Tutorial\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Probability Theory in Clinical Decision Making\n",
    "\n",
    "Probability theory forms the foundation of evidence-based medicine and clinical decision support systems. We'll explore how to apply probabilistic reasoning to diagnostic and therapeutic decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalProbabilityCalculator:\n",
    "    \"\"\"\n",
    "    Comprehensive probability calculator for clinical applications.\n",
    "    \n",
    "    This class implements various probability calculations commonly used\n",
    "    in clinical decision making, including diagnostic test interpretation\n",
    "    and risk assessment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the clinical probability calculator.\"\"\"\n",
    "        self.test_results = {}\n",
    "        \n",
    "    def calculate_diagnostic_accuracy(self, sensitivity, specificity, prevalence):\n",
    "        \"\"\"\n",
    "        Calculate diagnostic test accuracy metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sensitivity : float\n",
    "            Test sensitivity (true positive rate)\n",
    "        specificity : float\n",
    "            Test specificity (true negative rate)\n",
    "        prevalence : float\n",
    "            Disease prevalence in the population\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all diagnostic metrics\n",
    "        \"\"\"\n",
    "        # Calculate positive and negative predictive values\n",
    "        ppv = (sensitivity * prevalence) / (\n",
    "            sensitivity * prevalence + (1 - specificity) * (1 - prevalence)\n",
    "        )\n",
    "        \n",
    "        npv = (specificity * (1 - prevalence)) / (\n",
    "            specificity * (1 - prevalence) + (1 - sensitivity) * prevalence\n",
    "        )\n",
    "        \n",
    "        # Calculate likelihood ratios\n",
    "        lr_positive = sensitivity / (1 - specificity)\n",
    "        lr_negative = (1 - sensitivity) / specificity\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = sensitivity * prevalence + specificity * (1 - prevalence)\n",
    "        \n",
    "        return {\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'prevalence': prevalence,\n",
    "            'ppv': ppv,\n",
    "            'npv': npv,\n",
    "            'lr_positive': lr_positive,\n",
    "            'lr_negative': lr_negative,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "    \n",
    "    def bayesian_diagnostic_update(self, prior_probability, likelihood_ratio):\n",
    "        \"\"\"\n",
    "        Update diagnostic probability using Bayes' theorem.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prior_probability : float\n",
    "            Prior probability of disease (pre-test probability)\n",
    "        likelihood_ratio : float\n",
    "            Likelihood ratio of the test result\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Posterior probability (post-test probability)\n",
    "        \"\"\"\n",
    "        # Convert probability to odds\n",
    "        prior_odds = prior_probability / (1 - prior_probability)\n",
    "        \n",
    "        # Apply likelihood ratio\n",
    "        posterior_odds = prior_odds * likelihood_ratio\n",
    "        \n",
    "        # Convert back to probability\n",
    "        posterior_probability = posterior_odds / (1 + posterior_odds)\n",
    "        \n",
    "        return posterior_probability\n",
    "    \n",
    "    def plot_diagnostic_performance(self, sensitivity_range, specificity, prevalence):\n",
    "        \"\"\"\n",
    "        Plot how diagnostic performance varies with sensitivity.\n",
    "        \"\"\"\n",
    "        sensitivities = np.linspace(sensitivity_range[0], sensitivity_range[1], 100)\n",
    "        ppvs = []\n",
    "        npvs = []\n",
    "        \n",
    "        for sens in sensitivities:\n",
    "            metrics = self.calculate_diagnostic_accuracy(sens, specificity, prevalence)\n",
    "            ppvs.append(metrics['ppv'])\n",
    "            npvs.append(metrics['npv'])\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(sensitivities, ppvs, 'b-', linewidth=2, label='PPV')\n",
    "        plt.plot(sensitivities, npvs, 'r-', linewidth=2, label='NPV')\n",
    "        plt.xlabel('Sensitivity')\n",
    "        plt.ylabel('Predictive Value')\n",
    "        plt.title(f'Predictive Values vs Sensitivity\\n(Specificity={specificity:.2f}, Prevalence={prevalence:.2f})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ROC-like curve\n",
    "        plt.subplot(1, 2, 2)\n",
    "        fpr = 1 - specificity\n",
    "        plt.plot(sensitivities, [fpr] * len(sensitivities), 'g--', alpha=0.7, label='False Positive Rate')\n",
    "        plt.plot(sensitivities, sensitivities, 'b-', linewidth=2, label='True Positive Rate')\n",
    "        plt.xlabel('Sensitivity (True Positive Rate)')\n",
    "        plt.ylabel('Rate')\n",
    "        plt.title('Sensitivity vs False Positive Rate')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example: Diagnostic test evaluation\n",
    "calc = ClinicalProbabilityCalculator()\n",
    "\n",
    "# Example: COVID-19 rapid test\n",
    "covid_metrics = calc.calculate_diagnostic_accuracy(\n",
    "    sensitivity=0.85,  # 85% sensitivity\n",
    "    specificity=0.95,  # 95% specificity\n",
    "    prevalence=0.05    # 5% prevalence\n",
    ")\n",
    "\n",
    "print(\"COVID-19 Rapid Test Performance:\")\n",
    "print(f\"Sensitivity: {covid_metrics['sensitivity']:.1%}\")\n",
    "print(f\"Specificity: {covid_metrics['specificity']:.1%}\")\n",
    "print(f\"Positive Predictive Value: {covid_metrics['ppv']:.1%}\")\n",
    "print(f\"Negative Predictive Value: {covid_metrics['npv']:.1%}\")\n",
    "print(f\"Positive Likelihood Ratio: {covid_metrics['lr_positive']:.1f}\")\n",
    "print(f\"Negative Likelihood Ratio: {covid_metrics['lr_negative']:.2f}\")\n",
    "\n",
    "# Visualize performance\n",
    "calc.plot_diagnostic_performance(\n",
    "    sensitivity_range=(0.5, 1.0),\n",
    "    specificity=0.95,\n",
    "    prevalence=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Bayesian Inference for Clinical Decision Making\n",
    "\n",
    "Bayesian inference provides a principled framework for updating beliefs based on new evidence. This is particularly valuable in clinical settings where we need to combine prior knowledge with new test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianClinicalInference:\n",
    "    \"\"\"\n",
    "    Bayesian inference framework for clinical applications.\n",
    "    \n",
    "    This class implements Bayesian methods for clinical decision making,\n",
    "    including diagnostic reasoning and treatment effect estimation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Bayesian inference system.\"\"\"\n",
    "        self.prior_distributions = {}\n",
    "        self.posterior_distributions = {}\n",
    "        \n",
    "    def beta_binomial_conjugate(self, prior_alpha, prior_beta, successes, trials):\n",
    "        \"\"\"\n",
    "        Perform Beta-Binomial conjugate analysis.\n",
    "        \n",
    "        This is commonly used for analyzing treatment success rates,\n",
    "        diagnostic test performance, and other binary outcomes.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prior_alpha, prior_beta : float\n",
    "            Parameters of the Beta prior distribution\n",
    "        successes : int\n",
    "            Number of observed successes\n",
    "        trials : int\n",
    "            Total number of trials\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Posterior distribution parameters and statistics\n",
    "        \"\"\"\n",
    "        # Update parameters using conjugate prior\n",
    "        posterior_alpha = prior_alpha + successes\n",
    "        posterior_beta = prior_beta + trials - successes\n",
    "        \n",
    "        # Calculate statistics\n",
    "        prior_mean = prior_alpha / (prior_alpha + prior_beta)\n",
    "        posterior_mean = posterior_alpha / (posterior_alpha + posterior_beta)\n",
    "        \n",
    "        # Credible intervals\n",
    "        posterior_ci_lower = stats.beta.ppf(0.025, posterior_alpha, posterior_beta)\n",
    "        posterior_ci_upper = stats.beta.ppf(0.975, posterior_alpha, posterior_beta)\n",
    "        \n",
    "        return {\n",
    "            'prior_alpha': prior_alpha,\n",
    "            'prior_beta': prior_beta,\n",
    "            'posterior_alpha': posterior_alpha,\n",
    "            'posterior_beta': posterior_beta,\n",
    "            'prior_mean': prior_mean,\n",
    "            'posterior_mean': posterior_mean,\n",
    "            'credible_interval': (posterior_ci_lower, posterior_ci_upper),\n",
    "            'observed_rate': successes / trials if trials > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def normal_normal_conjugate(self, prior_mean, prior_var, data):\n",
    "        \"\"\"\n",
    "        Perform Normal-Normal conjugate analysis.\n",
    "        \n",
    "        Used for continuous outcomes like blood pressure, lab values, etc.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prior_mean : float\n",
    "            Prior mean\n",
    "        prior_var : float\n",
    "            Prior variance\n",
    "        data : array-like\n",
    "            Observed data points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Posterior distribution parameters and statistics\n",
    "        \"\"\"\n",
    "        data = np.array(data)\n",
    "        n = len(data)\n",
    "        sample_mean = np.mean(data)\n",
    "        \n",
    "        # Assume known variance for simplicity (can be extended)\n",
    "        data_var = np.var(data, ddof=1) if n > 1 else 1.0\n",
    "        \n",
    "        # Update parameters\n",
    "        posterior_var = 1 / (1/prior_var + n/data_var)\n",
    "        posterior_mean = posterior_var * (prior_mean/prior_var + n*sample_mean/data_var)\n",
    "        \n",
    "        # Credible intervals\n",
    "        posterior_std = np.sqrt(posterior_var)\n",
    "        ci_lower = stats.norm.ppf(0.025, posterior_mean, posterior_std)\n",
    "        ci_upper = stats.norm.ppf(0.975, posterior_mean, posterior_std)\n",
    "        \n",
    "        return {\n",
    "            'prior_mean': prior_mean,\n",
    "            'prior_var': prior_var,\n",
    "            'posterior_mean': posterior_mean,\n",
    "            'posterior_var': posterior_var,\n",
    "            'sample_mean': sample_mean,\n",
    "            'sample_size': n,\n",
    "            'credible_interval': (ci_lower, ci_upper)\n",
    "        }\n",
    "    \n",
    "    def plot_bayesian_update(self, prior_params, posterior_params, distribution_type='beta'):\n",
    "        \"\"\"\n",
    "        Visualize Bayesian updating process.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        if distribution_type == 'beta':\n",
    "            x = np.linspace(0, 1, 1000)\n",
    "            \n",
    "            # Prior distribution\n",
    "            prior_pdf = stats.beta.pdf(x, prior_params['prior_alpha'], prior_params['prior_beta'])\n",
    "            \n",
    "            # Posterior distribution\n",
    "            posterior_pdf = stats.beta.pdf(x, posterior_params['posterior_alpha'], posterior_params['posterior_beta'])\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(x, prior_pdf, 'b-', linewidth=2, label=f'Prior: Beta({prior_params[\"prior_alpha\"]}, {prior_params[\"prior_beta\"]})')\n",
    "            plt.plot(x, posterior_pdf, 'r-', linewidth=2, label=f'Posterior: Beta({posterior_params[\"posterior_alpha\"]}, {posterior_params[\"posterior_beta\"]})')\n",
    "            plt.axvline(posterior_params['observed_rate'], color='green', linestyle='--', label='Observed Rate')\n",
    "            plt.xlabel('Success Rate')\n",
    "            plt.ylabel('Density')\n",
    "            plt.title('Bayesian Update: Prior vs Posterior')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Credible interval visualization\n",
    "            plt.subplot(1, 2, 2)\n",
    "            ci_lower, ci_upper = posterior_params['credible_interval']\n",
    "            x_ci = x[(x >= ci_lower) & (x <= ci_upper)]\n",
    "            y_ci = stats.beta.pdf(x_ci, posterior_params['posterior_alpha'], posterior_params['posterior_beta'])\n",
    "            \n",
    "            plt.plot(x, posterior_pdf, 'r-', linewidth=2, label='Posterior Distribution')\n",
    "            plt.fill_between(x_ci, y_ci, alpha=0.3, color='red', label='95% Credible Interval')\n",
    "            plt.axvline(posterior_params['posterior_mean'], color='red', linestyle='-', label='Posterior Mean')\n",
    "            plt.xlabel('Success Rate')\n",
    "            plt.ylabel('Density')\n",
    "            plt.title(f'95% Credible Interval: [{ci_lower:.3f}, {ci_upper:.3f}]')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example: Treatment efficacy analysis\n",
    "bayesian_system = BayesianClinicalInference()\n",
    "\n",
    "# Example: New drug trial\n",
    "# Prior: Skeptical prior based on historical data\n",
    "prior_alpha = 2  # Equivalent to 2 prior successes\n",
    "prior_beta = 8   # Equivalent to 8 prior failures (20% success rate)\n",
    "\n",
    "# Observed data: 15 successes out of 20 patients\n",
    "successes = 15\n",
    "trials = 20\n",
    "\n",
    "# Perform Bayesian analysis\n",
    "drug_analysis = bayesian_system.beta_binomial_conjugate(\n",
    "    prior_alpha, prior_beta, successes, trials\n",
    ")\n",
    "\n",
    "print(\"Drug Efficacy Bayesian Analysis:\")\n",
    "print(f\"Prior belief (success rate): {drug_analysis['prior_mean']:.1%}\")\n",
    "print(f\"Observed success rate: {drug_analysis['observed_rate']:.1%}\")\n",
    "print(f\"Updated belief (posterior mean): {drug_analysis['posterior_mean']:.1%}\")\n",
    "print(f\"95% Credible Interval: [{drug_analysis['credible_interval'][0]:.1%}, {drug_analysis['credible_interval'][1]:.1%}]\")\n",
    "\n",
    "# Visualize the Bayesian update\n",
    "bayesian_system.plot_bayesian_update(drug_analysis, drug_analysis, 'beta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Information Theory in Healthcare Data Analysis\n",
    "\n",
    "Information theory provides powerful tools for analyzing healthcare data, including feature selection, data compression, and measuring the information content of diagnostic tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthcareInformationTheory:\n",
    "    \"\"\"\n",
    "    Information theory applications for healthcare data analysis.\n",
    "    \n",
    "    This class implements entropy, mutual information, and other\n",
    "    information-theoretic measures for clinical data analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize information theory calculator.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def calculate_entropy(self, probabilities):\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        probabilities : array-like\n",
    "            Probability distribution\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Entropy in bits\n",
    "        \"\"\"\n",
    "        probabilities = np.array(probabilities)\n",
    "        # Remove zero probabilities to avoid log(0)\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    def calculate_conditional_entropy(self, joint_probs):\n",
    "        \"\"\"\n",
    "        Calculate conditional entropy H(Y|X).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        joint_probs : 2D array\n",
    "            Joint probability distribution P(X,Y)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Conditional entropy\n",
    "        \"\"\"\n",
    "        joint_probs = np.array(joint_probs)\n",
    "        marginal_x = np.sum(joint_probs, axis=1)\n",
    "        \n",
    "        conditional_entropy = 0\n",
    "        for i in range(joint_probs.shape[0]):\n",
    "            if marginal_x[i] > 0:\n",
    "                conditional_probs = joint_probs[i, :] / marginal_x[i]\n",
    "                entropy_y_given_x = self.calculate_entropy(conditional_probs)\n",
    "                conditional_entropy += marginal_x[i] * entropy_y_given_x\n",
    "        \n",
    "        return conditional_entropy\n",
    "    \n",
    "    def calculate_mutual_information(self, joint_probs):\n",
    "        \"\"\"\n",
    "        Calculate mutual information I(X;Y).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        joint_probs : 2D array\n",
    "            Joint probability distribution P(X,Y)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Mutual information\n",
    "        \"\"\"\n",
    "        joint_probs = np.array(joint_probs)\n",
    "        marginal_x = np.sum(joint_probs, axis=1)\n",
    "        marginal_y = np.sum(joint_probs, axis=0)\n",
    "        \n",
    "        # Calculate entropies\n",
    "        entropy_x = self.calculate_entropy(marginal_x)\n",
    "        entropy_y = self.calculate_entropy(marginal_y)\n",
    "        joint_entropy = self.calculate_entropy(joint_probs.flatten())\n",
    "        \n",
    "        # Mutual information = H(X) + H(Y) - H(X,Y)\n",
    "        return entropy_x + entropy_y - joint_entropy\n",
    "    \n",
    "    def diagnostic_information_gain(self, disease_prevalence, test_sensitivity, test_specificity):\n",
    "        \"\"\"\n",
    "        Calculate information gain from a diagnostic test.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        disease_prevalence : float\n",
    "            Prior probability of disease\n",
    "        test_sensitivity : float\n",
    "            Test sensitivity\n",
    "        test_specificity : float\n",
    "            Test specificity\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Information gain analysis\n",
    "        \"\"\"\n",
    "        # Prior entropy (before test)\n",
    "        prior_probs = [disease_prevalence, 1 - disease_prevalence]\n",
    "        prior_entropy = self.calculate_entropy(prior_probs)\n",
    "        \n",
    "        # Joint probability distribution: P(Disease, Test Result)\n",
    "        # Rows: Disease (Present, Absent), Columns: Test (Positive, Negative)\n",
    "        joint_probs = np.array([\n",
    "            [disease_prevalence * test_sensitivity, disease_prevalence * (1 - test_sensitivity)],\n",
    "            [(1 - disease_prevalence) * (1 - test_specificity), (1 - disease_prevalence) * test_specificity]\n",
    "        ])\n",
    "        \n",
    "        # Calculate mutual information\n",
    "        mutual_info = self.calculate_mutual_information(joint_probs)\n",
    "        \n",
    "        # Information gain = Mutual information\n",
    "        information_gain = mutual_info\n",
    "        \n",
    "        # Relative information gain\n",
    "        relative_gain = information_gain / prior_entropy if prior_entropy > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'prior_entropy': prior_entropy,\n",
    "            'information_gain': information_gain,\n",
    "            'relative_gain': relative_gain,\n",
    "            'joint_distribution': joint_probs\n",
    "        }\n",
    "    \n",
    "    def feature_selection_mutual_information(self, X, y):\n",
    "        \"\"\"\n",
    "        Rank features by mutual information with target variable.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Feature matrix\n",
    "        y : array-like\n",
    "            Target variable\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame : Features ranked by mutual information\n",
    "        \"\"\"\n",
    "        from sklearn.feature_selection import mutual_info_classif\n",
    "        \n",
    "        # Calculate mutual information for each feature\n",
    "        mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        feature_ranking = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'mutual_information': mi_scores\n",
    "        }).sort_values('mutual_information', ascending=False)\n",
    "        \n",
    "        return feature_ranking\n",
    "    \n",
    "    def plot_information_analysis(self, prevalence_range, sensitivity, specificity):\n",
    "        \"\"\"\n",
    "        Plot information gain across different disease prevalences.\n",
    "        \"\"\"\n",
    "        prevalences = np.linspace(prevalence_range[0], prevalence_range[1], 100)\n",
    "        information_gains = []\n",
    "        prior_entropies = []\n",
    "        \n",
    "        for prev in prevalences:\n",
    "            analysis = self.diagnostic_information_gain(prev, sensitivity, specificity)\n",
    "            information_gains.append(analysis['information_gain'])\n",
    "            prior_entropies.append(analysis['prior_entropy'])\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(prevalences, information_gains, 'b-', linewidth=2, label='Information Gain')\n",
    "        plt.plot(prevalences, prior_entropies, 'r--', linewidth=2, label='Prior Entropy')\n",
    "        plt.xlabel('Disease Prevalence')\n",
    "        plt.ylabel('Information (bits)')\n",
    "        plt.title(f'Information Gain vs Disease Prevalence\\n(Sensitivity={sensitivity:.2f}, Specificity={specificity:.2f})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        relative_gains = np.array(information_gains) / np.array(prior_entropies)\n",
    "        plt.plot(prevalences, relative_gains, 'g-', linewidth=2)\n",
    "        plt.xlabel('Disease Prevalence')\n",
    "        plt.ylabel('Relative Information Gain')\n",
    "        plt.title('Relative Information Gain')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example: Information theory analysis\n",
    "info_theory = HealthcareInformationTheory()\n",
    "\n",
    "# Example: Diagnostic test information analysis\n",
    "test_analysis = info_theory.diagnostic_information_gain(\n",
    "    disease_prevalence=0.1,\n",
    "    test_sensitivity=0.9,\n",
    "    test_specificity=0.95\n",
    ")\n",
    "\n",
    "print(\"Diagnostic Test Information Analysis:\")\n",
    "print(f\"Prior entropy (uncertainty): {test_analysis['prior_entropy']:.3f} bits\")\n",
    "print(f\"Information gain from test: {test_analysis['information_gain']:.3f} bits\")\n",
    "print(f\"Relative information gain: {test_analysis['relative_gain']:.1%}\")\n",
    "\n",
    "# Visualize information gain across prevalences\n",
    "info_theory.plot_information_analysis(\n",
    "    prevalence_range=(0.01, 0.5),\n",
    "    sensitivity=0.9,\n",
    "    specificity=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Optimization Techniques for Clinical Machine Learning\n",
    "\n",
    "Optimization is at the heart of machine learning algorithms. We'll explore optimization techniques specifically relevant to healthcare applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalOptimization:\n",
    "    \"\"\"\n",
    "    Optimization techniques for clinical machine learning applications.\n",
    "    \n",
    "    This class implements various optimization algorithms with healthcare-specific\n",
    "    considerations such as class imbalance and clinical constraints.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize optimization framework.\"\"\"\n",
    "        self.optimization_history = []\n",
    "        \n",
    "    def logistic_regression_gradient_descent(self, X, y, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Implement logistic regression using gradient descent.\n",
    "        \n",
    "        This demonstrates the optimization process for a common clinical prediction model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix\n",
    "        y : array-like\n",
    "            Binary target variable\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "        max_iterations : int\n",
    "            Maximum number of iterations\n",
    "        tolerance : float\n",
    "            Convergence tolerance\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Optimization results including weights and convergence history\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Add intercept term\n",
    "        X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        \n",
    "        # Initialize weights\n",
    "        weights = np.random.normal(0, 0.01, X_with_intercept.shape[1])\n",
    "        \n",
    "        # Track optimization history\n",
    "        cost_history = []\n",
    "        weight_history = []\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            # Forward pass\n",
    "            z = X_with_intercept @ weights\n",
    "            predictions = 1 / (1 + np.exp(-z))  # Sigmoid function\n",
    "            \n",
    "            # Calculate cost (log-likelihood)\n",
    "            cost = -np.mean(y * np.log(predictions + 1e-15) + (1 - y) * np.log(1 - predictions + 1e-15))\n",
    "            cost_history.append(cost)\n",
    "            weight_history.append(weights.copy())\n",
    "            \n",
    "            # Calculate gradients\n",
    "            gradients = X_with_intercept.T @ (predictions - y) / len(y)\n",
    "            \n",
    "            # Update weights\n",
    "            new_weights = weights - learning_rate * gradients\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(new_weights - weights) < tolerance:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "            \n",
    "            weights = new_weights\n",
    "        \n",
    "        return {\n",
    "            'weights': weights,\n",
    "            'cost_history': cost_history,\n",
    "            'weight_history': weight_history,\n",
    "            'final_cost': cost_history[-1],\n",
    "            'iterations': len(cost_history)\n",
    "        }\n",
    "    \n",
    "    def clinical_loss_function(self, y_true, y_pred, false_negative_cost=5, false_positive_cost=1):\n",
    "        \"\"\"\n",
    "        Custom loss function that accounts for clinical costs.\n",
    "        \n",
    "        In healthcare, false negatives (missing a disease) often have\n",
    "        higher costs than false positives (unnecessary treatment).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like\n",
    "            True labels\n",
    "        y_pred : array-like\n",
    "            Predicted probabilities\n",
    "        false_negative_cost : float\n",
    "            Cost of false negative (missing disease)\n",
    "        false_positive_cost : float\n",
    "            Cost of false positive (unnecessary treatment)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Weighted loss\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "        # Calculate weighted cross-entropy loss\n",
    "        false_negative_loss = -false_negative_cost * y_true * np.log(y_pred + 1e-15)\n",
    "        false_positive_loss = -false_positive_cost * (1 - y_true) * np.log(1 - y_pred + 1e-15)\n",
    "        \n",
    "        return np.mean(false_negative_loss + false_positive_loss)\n",
    "    \n",
    "    def optimize_clinical_threshold(self, y_true, y_pred_proba, false_negative_cost=5, false_positive_cost=1):\n",
    "        \"\"\"\n",
    "        Optimize decision threshold based on clinical costs.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like\n",
    "            True labels\n",
    "        y_pred_proba : array-like\n",
    "            Predicted probabilities\n",
    "        false_negative_cost : float\n",
    "            Cost of false negative\n",
    "        false_positive_cost : float\n",
    "            Cost of false positive\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Optimal threshold and cost analysis\n",
    "        \"\"\"\n",
    "        thresholds = np.linspace(0.01, 0.99, 100)\n",
    "        costs = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "            \n",
    "            # Calculate confusion matrix elements\n",
    "            tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "            fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "            fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "            tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "            \n",
    "            # Calculate total cost\n",
    "            total_cost = fn * false_negative_cost + fp * false_positive_cost\n",
    "            costs.append(total_cost)\n",
    "        \n",
    "        # Find optimal threshold\n",
    "        optimal_idx = np.argmin(costs)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        optimal_cost = costs[optimal_idx]\n",
    "        \n",
    "        return {\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'optimal_cost': optimal_cost,\n",
    "            'thresholds': thresholds,\n",
    "            'costs': costs\n",
    "        }\n",
    "    \n",
    "    def plot_optimization_results(self, optimization_results, threshold_results=None):\n",
    "        \"\"\"\n",
    "        Visualize optimization results.\n",
    "        \"\"\"\n",
    "        if threshold_results is not None:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Plot cost convergence\n",
    "        axes[0].plot(optimization_results['cost_history'], 'b-', linewidth=2)\n",
    "        axes[0].set_xlabel('Iteration')\n",
    "        axes[0].set_ylabel('Cost')\n",
    "        axes[0].set_title('Cost Function Convergence')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot weight evolution\n",
    "        weight_history = np.array(optimization_results['weight_history'])\n",
    "        for i in range(weight_history.shape[1]):\n",
    "            axes[1].plot(weight_history[:, i], label=f'Weight {i}', alpha=0.7)\n",
    "        axes[1].set_xlabel('Iteration')\n",
    "        axes[1].set_ylabel('Weight Value')\n",
    "        axes[1].set_title('Weight Evolution During Training')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot threshold optimization if provided\n",
    "        if threshold_results is not None:\n",
    "            axes[2].plot(threshold_results['thresholds'], threshold_results['costs'], 'r-', linewidth=2)\n",
    "            axes[2].axvline(threshold_results['optimal_threshold'], color='green', linestyle='--', \n",
    "                           label=f'Optimal: {threshold_results[\"optimal_threshold\"]:.3f}')\n",
    "            axes[2].set_xlabel('Decision Threshold')\n",
    "            axes[2].set_ylabel('Total Clinical Cost')\n",
    "            axes[2].set_title('Clinical Cost vs Decision Threshold')\n",
    "            axes[2].legend()\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Generate example clinical data for optimization\n",
    "np.random.seed(42)\n",
    "n_patients = 1000\n",
    "\n",
    "# Simulate clinical features\n",
    "age = np.random.normal(65, 15, n_patients)\n",
    "bmi = np.random.normal(28, 5, n_patients)\n",
    "glucose = np.random.normal(120, 30, n_patients)\n",
    "bp_systolic = np.random.normal(140, 20, n_patients)\n",
    "\n",
    "# Create feature matrix\n",
    "X_clinical = np.column_stack([age, bmi, glucose, bp_systolic])\n",
    "\n",
    "# Generate outcome (e.g., diabetes diagnosis)\n",
    "# Higher risk with age, BMI, glucose, and blood pressure\n",
    "risk_score = 0.02 * age + 0.1 * bmi + 0.01 * glucose + 0.005 * bp_systolic - 8\n",
    "probability = 1 / (1 + np.exp(-risk_score))\n",
    "y_clinical = np.random.binomial(1, probability, n_patients)\n",
    "\n",
    "print(f\"Clinical dataset: {n_patients} patients, {X_clinical.shape[1]} features\")\n",
    "print(f\"Outcome prevalence: {y_clinical.mean():.1%}\")\n",
    "\n",
    "# Perform optimization\n",
    "optimizer = ClinicalOptimization()\n",
    "\n",
    "# Train model using gradient descent\n",
    "optimization_results = optimizer.logistic_regression_gradient_descent(\n",
    "    X_clinical, y_clinical, learning_rate=0.001, max_iterations=1000\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimization completed in {optimization_results['iterations']} iterations\")\n",
    "print(f\"Final cost: {optimization_results['final_cost']:.4f}\")\n",
    "print(f\"Final weights: {optimization_results['weights']}\")\n",
    "\n",
    "# Generate predictions for threshold optimization\n",
    "X_with_intercept = np.column_stack([np.ones(X_clinical.shape[0]), X_clinical])\n",
    "y_pred_proba = 1 / (1 + np.exp(-(X_with_intercept @ optimization_results['weights'])))\n",
    "\n",
    "# Optimize clinical threshold\n",
    "threshold_results = optimizer.optimize_clinical_threshold(\n",
    "    y_clinical, y_pred_proba, false_negative_cost=5, false_positive_cost=1\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimal clinical threshold: {threshold_results['optimal_threshold']:.3f}\")\n",
    "print(f\"Optimal total cost: {threshold_results['optimal_cost']:.0f}\")\n",
    "\n",
    "# Visualize results\n",
    "optimizer.plot_optimization_results(optimization_results, threshold_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Causal Inference for Healthcare Research\n",
    "\n",
    "Causal inference is crucial for understanding treatment effects and making evidence-based clinical decisions. We'll implement key causal inference methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalInferenceHealthcare:\n",
    "    \"\"\"\n",
    "    Causal inference methods for healthcare research.\n",
    "    \n",
    "    This class implements various causal inference techniques including\n",
    "    propensity score matching, instrumental variables, and difference-in-differences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize causal inference framework.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def propensity_score_matching(self, X, treatment, outcome, caliper=0.1):\n",
    "        \"\"\"\n",
    "        Perform propensity score matching for causal inference.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Covariate matrix\n",
    "        treatment : array-like\n",
    "            Binary treatment indicator\n",
    "        outcome : array-like\n",
    "            Outcome variable\n",
    "        caliper : float\n",
    "            Maximum distance for matching\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Matching results and treatment effect estimate\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        \n",
    "        # Estimate propensity scores\n",
    "        ps_model = LogisticRegression(random_state=42)\n",
    "        ps_model.fit(X, treatment)\n",
    "        propensity_scores = ps_model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Separate treated and control groups\n",
    "        treated_idx = np.where(treatment == 1)[0]\n",
    "        control_idx = np.where(treatment == 0)[0]\n",
    "        \n",
    "        # Perform nearest neighbor matching\n",
    "        nn = NearestNeighbors(n_neighbors=1, metric='euclidean')\n",
    "        nn.fit(propensity_scores[control_idx].reshape(-1, 1))\n",
    "        \n",
    "        matched_pairs = []\n",
    "        for treated_i in treated_idx:\n",
    "            distances, indices = nn.kneighbors(\n",
    "                propensity_scores[treated_i].reshape(1, -1)\n",
    "            )\n",
    "            \n",
    "            if distances[0][0] <= caliper:\n",
    "                control_match_idx = control_idx[indices[0][0]]\n",
    "                matched_pairs.append((treated_i, control_match_idx))\n",
    "        \n",
    "        # Calculate treatment effect\n",
    "        if len(matched_pairs) > 0:\n",
    "            treated_outcomes = [outcome[pair[0]] for pair in matched_pairs]\n",
    "            control_outcomes = [outcome[pair[1]] for pair in matched_pairs]\n",
    "            \n",
    "            ate = np.mean(treated_outcomes) - np.mean(control_outcomes)\n",
    "            \n",
    "            # Calculate standard error\n",
    "            differences = np.array(treated_outcomes) - np.array(control_outcomes)\n",
    "            se = np.std(differences) / np.sqrt(len(differences))\n",
    "            \n",
    "            # 95% confidence interval\n",
    "            ci_lower = ate - 1.96 * se\n",
    "            ci_upper = ate + 1.96 * se\n",
    "        else:\n",
    "            ate = np.nan\n",
    "            se = np.nan\n",
    "            ci_lower = np.nan\n",
    "            ci_upper = np.nan\n",
    "        \n",
    "        return {\n",
    "            'propensity_scores': propensity_scores,\n",
    "            'matched_pairs': matched_pairs,\n",
    "            'n_matched': len(matched_pairs),\n",
    "            'ate': ate,\n",
    "            'standard_error': se,\n",
    "            'confidence_interval': (ci_lower, ci_upper),\n",
    "            'ps_model': ps_model\n",
    "        }\n",
    "    \n",
    "    def instrumental_variable_analysis(self, X, treatment, outcome, instrument):\n",
    "        \"\"\"\n",
    "        Perform instrumental variable analysis using two-stage least squares.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Covariate matrix\n",
    "        treatment : array-like\n",
    "            Treatment variable (can be continuous)\n",
    "        outcome : array-like\n",
    "            Outcome variable\n",
    "        instrument : array-like\n",
    "            Instrumental variable\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : IV analysis results\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        \n",
    "        # First stage: regress treatment on instrument and covariates\n",
    "        X_with_instrument = np.column_stack([X, instrument])\n",
    "        first_stage = LinearRegression()\n",
    "        first_stage.fit(X_with_instrument, treatment)\n",
    "        predicted_treatment = first_stage.predict(X_with_instrument)\n",
    "        \n",
    "        # Check instrument strength (F-statistic)\n",
    "        residuals_first = treatment - predicted_treatment\n",
    "        f_stat = np.var(predicted_treatment) / np.var(residuals_first) * (len(treatment) - X_with_instrument.shape[1] - 1)\n",
    "        \n",
    "        # Second stage: regress outcome on predicted treatment and covariates\n",
    "        X_with_predicted_treatment = np.column_stack([X, predicted_treatment])\n",
    "        second_stage = LinearRegression()\n",
    "        second_stage.fit(X_with_predicted_treatment, outcome)\n",
    "        \n",
    "        # Treatment effect is the coefficient on predicted treatment\n",
    "        treatment_effect = second_stage.coef_[-1]\n",
    "        \n",
    "        return {\n",
    "            'treatment_effect': treatment_effect,\n",
    "            'first_stage_model': first_stage,\n",
    "            'second_stage_model': second_stage,\n",
    "            'f_statistic': f_stat,\n",
    "            'instrument_strength': 'Strong' if f_stat > 10 else 'Weak',\n",
    "            'predicted_treatment': predicted_treatment\n",
    "        }\n",
    "    \n",
    "    def plot_causal_analysis(self, ps_results, iv_results=None):\n",
    "        \"\"\"\n",
    "        Visualize causal analysis results.\n",
    "        \"\"\"\n",
    "        if iv_results is not None:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        # Propensity score distribution\n",
    "        treated_ps = ps_results['propensity_scores'][treatment == 1]\n",
    "        control_ps = ps_results['propensity_scores'][treatment == 0]\n",
    "        \n",
    "        axes[0, 0].hist(control_ps, bins=30, alpha=0.7, label='Control', color='blue')\n",
    "        axes[0, 0].hist(treated_ps, bins=30, alpha=0.7, label='Treated', color='red')\n",
    "        axes[0, 0].set_xlabel('Propensity Score')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Propensity Score Distribution')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Treatment effect visualization\n",
    "        ate = ps_results['ate']\n",
    "        ci_lower, ci_upper = ps_results['confidence_interval']\n",
    "        \n",
    "        axes[0, 1].errorbar([0], [ate], yerr=[[ate - ci_lower], [ci_upper - ate]], \n",
    "                           fmt='o', capsize=10, capthick=2, markersize=8, color='red')\n",
    "        axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[0, 1].set_xlim(-0.5, 0.5)\n",
    "        axes[0, 1].set_ylabel('Treatment Effect')\n",
    "        axes[0, 1].set_title(f'Average Treatment Effect\\n{ate:.3f} [{ci_lower:.3f}, {ci_upper:.3f}]')\n",
    "        axes[0, 1].set_xticks([])\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # IV analysis plots if provided\n",
    "        if iv_results is not None:\n",
    "            # First stage relationship\n",
    "            axes[1, 0].scatter(instrument, treatment, alpha=0.6)\n",
    "            axes[1, 0].plot(instrument, iv_results['predicted_treatment'], 'r-', linewidth=2)\n",
    "            axes[1, 0].set_xlabel('Instrument')\n",
    "            axes[1, 0].set_ylabel('Treatment')\n",
    "            axes[1, 0].set_title(f'First Stage (F-stat: {iv_results[\"f_statistic\"]:.1f})')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # IV treatment effect\n",
    "            iv_effect = iv_results['treatment_effect']\n",
    "            axes[1, 1].bar(['PS Matching', 'IV Estimate'], [ate, iv_effect], \n",
    "                          color=['blue', 'green'], alpha=0.7)\n",
    "            axes[1, 1].set_ylabel('Treatment Effect')\n",
    "            axes[1, 1].set_title('Comparison of Causal Estimates')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Generate example data for causal inference\n",
    "np.random.seed(42)\n",
    "n_patients = 2000\n",
    "\n",
    "# Confounders (age, comorbidity score, severity)\n",
    "age = np.random.normal(65, 15, n_patients)\n",
    "comorbidity_score = np.random.poisson(2, n_patients)\n",
    "severity = np.random.normal(0, 1, n_patients)\n",
    "\n",
    "# Instrumental variable (e.g., physician preference, distance to specialist)\n",
    "instrument = np.random.normal(0, 1, n_patients)\n",
    "\n",
    "# Treatment assignment (influenced by confounders and instrument)\n",
    "treatment_logit = (\n",
    "    0.02 * age + \n",
    "    0.3 * comorbidity_score + \n",
    "    0.4 * severity + \n",
    "    0.5 * instrument +  # Instrument effect\n",
    "    np.random.normal(0, 1, n_patients)\n",
    ")\n",
    "treatment_prob = 1 / (1 + np.exp(-treatment_logit))\n",
    "treatment = np.random.binomial(1, treatment_prob, n_patients)\n",
    "\n",
    "# Outcome (influenced by confounders and treatment)\n",
    "true_treatment_effect = 2.0  # True causal effect\n",
    "outcome = (\n",
    "    0.1 * age + \n",
    "    0.5 * comorbidity_score + \n",
    "    1.0 * severity + \n",
    "    true_treatment_effect * treatment +  # True causal effect\n",
    "    np.random.normal(0, 2, n_patients)\n",
    ")\n",
    "\n",
    "# Create covariate matrix\n",
    "X_causal = pd.DataFrame({\n",
    "    'age': age,\n",
    "    'comorbidity_score': comorbidity_score,\n",
    "    'severity': severity\n",
    "})\n",
    "\n",
    "print(f\"Causal inference dataset: {n_patients} patients\")\n",
    "print(f\"Treatment rate: {treatment.mean():.1%}\")\n",
    "print(f\"True treatment effect: {true_treatment_effect}\")\n",
    "\n",
    "# Perform causal analysis\n",
    "causal_analyzer = CausalInferenceHealthcare()\n",
    "\n",
    "# Propensity score matching\n",
    "ps_results = causal_analyzer.propensity_score_matching(\n",
    "    X_causal, treatment, outcome, caliper=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nPropensity Score Matching Results:\")\n",
    "print(f\"Matched pairs: {ps_results['n_matched']} out of {np.sum(treatment)} treated patients\")\n",
    "print(f\"Estimated ATE: {ps_results['ate']:.3f} (SE: {ps_results['standard_error']:.3f})\")\n",
    "print(f\"95% CI: [{ps_results['confidence_interval'][0]:.3f}, {ps_results['confidence_interval'][1]:.3f}]\")\n",
    "\n",
    "# Instrumental variable analysis\n",
    "iv_results = causal_analyzer.instrumental_variable_analysis(\n",
    "    X_causal, treatment, outcome, instrument\n",
    ")\n",
    "\n",
    "print(f\"\\nInstrumental Variable Analysis Results:\")\n",
    "print(f\"Estimated treatment effect: {iv_results['treatment_effect']:.3f}\")\n",
    "print(f\"First-stage F-statistic: {iv_results['f_statistic']:.1f} ({iv_results['instrument_strength']} instrument)\")\n",
    "\n",
    "# Visualize results\n",
    "causal_analyzer.plot_causal_analysis(ps_results, iv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this chapter, we've covered the essential mathematical foundations for healthcare AI:\n",
    "\n",
    "1. **Probability Theory**: Diagnostic test interpretation and clinical decision making\n",
    "2. **Bayesian Inference**: Updating beliefs with new evidence\n",
    "3. **Information Theory**: Measuring information content and feature selection\n",
    "4. **Optimization**: Training clinical prediction models\n",
    "5. **Causal Inference**: Estimating treatment effects from observational data\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Clinical Context Matters**: Mathematical methods must be adapted for healthcare-specific considerations like class imbalance, clinical costs, and regulatory requirements\n",
    "- **Uncertainty Quantification**: Healthcare decisions require proper uncertainty estimation and confidence intervals\n",
    "- **Causal Reasoning**: Correlation is not causation - proper causal inference methods are essential for treatment effect estimation\n",
    "- **Interpretability**: Mathematical models in healthcare must be interpretable and explainable to clinicians\n",
    "\n",
    "### Next Chapter Preview:\n",
    "\n",
    "Chapter 3 will build on these mathematical foundations to cover **Healthcare Data Engineering**, including:\n",
    "- FHIR and HL7 standards implementation\n",
    "- Real-time data processing pipelines\n",
    "- Data quality and validation frameworks\n",
    "- Privacy-preserving data processing\n",
    "\n",
    "### Exercises:\n",
    "\n",
    "1. Modify the diagnostic test calculator to handle multiple tests in sequence\n",
    "2. Implement a Bayesian A/B test framework for clinical trials\n",
    "3. Create a feature selection pipeline using mutual information\n",
    "4. Develop a cost-sensitive optimization algorithm for imbalanced clinical data\n",
    "5. Implement difference-in-differences analysis for policy evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
