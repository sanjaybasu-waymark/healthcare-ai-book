{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Healthcare Data Engineering\n",
    "\n",
    "## Interactive Tutorial and Hands-on Examples\n",
    "\n",
    "This notebook provides comprehensive, hands-on examples for healthcare data engineering concepts covered in Chapter 3 of the Healthcare AI Implementation Guide.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Process FHIR resources** for healthcare interoperability\n",
    "2. **Build real-time data pipelines** for streaming healthcare data\n",
    "3. **Implement data quality frameworks** with healthcare-specific validation\n",
    "4. **Design scalable data architectures** for healthcare AI applications\n",
    "5. **Handle privacy and security** requirements in healthcare data processing\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Basic Python programming knowledge\n",
    "- Understanding of healthcare data formats (HL7, FHIR)\n",
    "- Familiarity with pandas and data processing concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import asyncio\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: FHIR Resource Processing\n",
    "\n",
    "FHIR (Fast Healthcare Interoperability Resources) is the modern standard for healthcare data exchange. Let's build a comprehensive FHIR processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FHIRProcessor:\n",
    "    \"\"\"\n",
    "    Comprehensive FHIR data processor for healthcare interoperability.\n",
    "    \n",
    "    This class handles FHIR resource processing, validation, and transformation\n",
    "    for healthcare AI applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize FHIR processor.\"\"\"\n",
    "        self.processed_resources = []\n",
    "        self.validation_errors = []\n",
    "        \n",
    "        # FHIR resource schemas (simplified)\n",
    "        self.resource_schemas = {\n",
    "            'Patient': {\n",
    "                'required': ['resourceType', 'id'],\n",
    "                'optional': ['identifier', 'name', 'gender', 'birthDate', 'address']\n",
    "            },\n",
    "            'Observation': {\n",
    "                'required': ['resourceType', 'status', 'code', 'subject'],\n",
    "                'optional': ['valueQuantity', 'valueString', 'effectiveDateTime']\n",
    "            },\n",
    "            'Condition': {\n",
    "                'required': ['resourceType', 'subject', 'code'],\n",
    "                'optional': ['clinicalStatus', 'verificationStatus', 'onsetDateTime']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def validate_fhir_resource(self, resource_data: Dict[str, Any]) -> tuple[bool, List[str]]:\n",
    "        \"\"\"Validate FHIR resource against schema.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Check if resourceType exists\n",
    "        if 'resourceType' not in resource_data:\n",
    "            errors.append(\"Missing required field: resourceType\")\n",
    "            return False, errors\n",
    "        \n",
    "        resource_type = resource_data['resourceType']\n",
    "        \n",
    "        # Check if we have a schema for this resource type\n",
    "        if resource_type not in self.resource_schemas:\n",
    "            errors.append(f\"Unknown resource type: {resource_type}\")\n",
    "            return False, errors\n",
    "        \n",
    "        schema = self.resource_schemas[resource_type]\n",
    "        \n",
    "        # Check required fields\n",
    "        for field in schema['required']:\n",
    "            if field not in resource_data:\n",
    "                errors.append(f\"Missing required field: {field}\")\n",
    "        \n",
    "        # Validate specific field formats\n",
    "        if resource_type == 'Patient':\n",
    "            if 'birthDate' in resource_data:\n",
    "                try:\n",
    "                    datetime.strptime(resource_data['birthDate'], '%Y-%m-%d')\n",
    "                except ValueError:\n",
    "                    errors.append(\"Invalid birthDate format. Expected YYYY-MM-DD\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    def process_fhir_bundle(self, bundle_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a FHIR bundle containing multiple resources.\"\"\"\n",
    "        results = {\n",
    "            'processed_count': 0,\n",
    "            'error_count': 0,\n",
    "            'resources_by_type': {},\n",
    "            'validation_errors': []\n",
    "        }\n",
    "        \n",
    "        if 'entry' not in bundle_data:\n",
    "            results['validation_errors'].append(\"Bundle missing 'entry' field\")\n",
    "            return results\n",
    "        \n",
    "        for entry in bundle_data['entry']:\n",
    "            if 'resource' not in entry:\n",
    "                results['error_count'] += 1\n",
    "                results['validation_errors'].append(\"Bundle entry missing 'resource' field\")\n",
    "                continue\n",
    "            \n",
    "            resource = entry['resource']\n",
    "            is_valid, errors = self.validate_fhir_resource(resource)\n",
    "            \n",
    "            if is_valid:\n",
    "                resource_type = resource['resourceType']\n",
    "                if resource_type not in results['resources_by_type']:\n",
    "                    results['resources_by_type'][resource_type] = []\n",
    "                \n",
    "                results['resources_by_type'][resource_type].append(resource)\n",
    "                results['processed_count'] += 1\n",
    "            else:\n",
    "                results['error_count'] += 1\n",
    "                results['validation_errors'].extend(errors)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize FHIR processor\n",
    "fhir_processor = FHIRProcessor()\n",
    "print(\"✓ FHIR Processor initialized\")\n",
    "print(f\"✓ Supported resource types: {list(fhir_processor.resource_schemas.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sample FHIR Data\n",
    "\n",
    "Let's create some sample FHIR resources to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample FHIR bundle\n",
    "sample_bundle = {\n",
    "    \"resourceType\": \"Bundle\",\n",
    "    \"id\": \"example-bundle\",\n",
    "    \"type\": \"collection\",\n",
    "    \"entry\": [\n",
    "        {\n",
    "            \"resource\": {\n",
    "                \"resourceType\": \"Patient\",\n",
    "                \"id\": \"patient-001\",\n",
    "                \"identifier\": [\n",
    "                    {\n",
    "                        \"system\": \"http://hospital.example.com/patient-ids\",\n",
    "                        \"value\": \"MRN123456\"\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": [\n",
    "                    {\n",
    "                        \"family\": \"Smith\",\n",
    "                        \"given\": [\"John\"]\n",
    "                    }\n",
    "                ],\n",
    "                \"gender\": \"male\",\n",
    "                \"birthDate\": \"1980-01-15\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"resource\": {\n",
    "                \"resourceType\": \"Observation\",\n",
    "                \"id\": \"obs-001\",\n",
    "                \"status\": \"final\",\n",
    "                \"code\": {\n",
    "                    \"coding\": [\n",
    "                        {\n",
    "                            \"system\": \"http://loinc.org\",\n",
    "                            \"code\": \"8480-6\",\n",
    "                            \"display\": \"Systolic blood pressure\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"subject\": {\n",
    "                    \"reference\": \"Patient/patient-001\"\n",
    "                },\n",
    "                \"effectiveDateTime\": \"2024-01-15T10:30:00Z\",\n",
    "                \"valueQuantity\": {\n",
    "                    \"value\": 120,\n",
    "                    \"unit\": \"mmHg\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"resource\": {\n",
    "                \"resourceType\": \"Condition\",\n",
    "                \"id\": \"condition-001\",\n",
    "                \"subject\": {\n",
    "                    \"reference\": \"Patient/patient-001\"\n",
    "                },\n",
    "                \"code\": {\n",
    "                    \"coding\": [\n",
    "                        {\n",
    "                            \"system\": \"http://snomed.info/sct\",\n",
    "                            \"code\": \"38341003\",\n",
    "                            \"display\": \"Hypertension\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"clinicalStatus\": {\n",
    "                    \"coding\": [\n",
    "                        {\n",
    "                            \"system\": \"http://terminology.hl7.org/CodeSystem/condition-clinical\",\n",
    "                            \"code\": \"active\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"✓ Sample FHIR bundle created\")\n",
    "print(f\"✓ Bundle contains {len(sample_bundle['entry'])} resources\")\n",
    "\n",
    "# Display the bundle structure\n",
    "print(\"\\nBundle structure:\")\n",
    "for i, entry in enumerate(sample_bundle['entry']):\n",
    "    resource = entry['resource']\n",
    "    print(f\"  {i+1}. {resource['resourceType']} (ID: {resource['id']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing FHIR Bundle\n",
    "\n",
    "Now let's process the FHIR bundle and validate the resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the FHIR bundle\n",
    "bundle_results = fhir_processor.process_fhir_bundle(sample_bundle)\n",
    "\n",
    "print(\"FHIR Bundle Processing Results:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"✓ Successfully processed: {bundle_results['processed_count']} resources\")\n",
    "print(f\"✗ Errors encountered: {bundle_results['error_count']} resources\")\n",
    "print(f\"📊 Resource types found: {list(bundle_results['resources_by_type'].keys())}\")\n",
    "\n",
    "# Display resource counts by type\n",
    "print(\"\\nResource counts by type:\")\n",
    "for resource_type, resources in bundle_results['resources_by_type'].items():\n",
    "    print(f\"  • {resource_type}: {len(resources)} resources\")\n",
    "\n",
    "# Display any validation errors\n",
    "if bundle_results['validation_errors']:\n",
    "    print(\"\\n⚠️ Validation errors:\")\n",
    "    for error in bundle_results['validation_errors']:\n",
    "        print(f\"  • {error}\")\n",
    "else:\n",
    "    print(\"\\n✅ No validation errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting FHIR to Tabular Format\n",
    "\n",
    "For machine learning applications, we often need to convert FHIR resources to tabular format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fhir_to_tabular(resources: List[Dict[str, Any]], resource_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Transform FHIR resources to tabular format.\"\"\"\n",
    "    \n",
    "    if resource_type == 'Patient':\n",
    "        patient_data = []\n",
    "        \n",
    "        for patient in resources:\n",
    "            row = {\n",
    "                'patient_id': patient.get('id', ''),\n",
    "                'gender': patient.get('gender', ''),\n",
    "                'birth_date': patient.get('birthDate', ''),\n",
    "            }\n",
    "            \n",
    "            # Extract name\n",
    "            if 'name' in patient and len(patient['name']) > 0:\n",
    "                name = patient['name'][0]\n",
    "                if 'family' in name:\n",
    "                    row['family_name'] = name['family']\n",
    "                if 'given' in name and len(name['given']) > 0:\n",
    "                    row['given_name'] = name['given'][0]\n",
    "            \n",
    "            # Extract identifiers\n",
    "            if 'identifier' in patient:\n",
    "                for identifier in patient['identifier']:\n",
    "                    if identifier.get('system') == 'http://hospital.example.com/patient-ids':\n",
    "                        row['mrn'] = identifier.get('value', '')\n",
    "            \n",
    "            patient_data.append(row)\n",
    "        \n",
    "        return pd.DataFrame(patient_data)\n",
    "    \n",
    "    elif resource_type == 'Observation':\n",
    "        obs_data = []\n",
    "        \n",
    "        for obs in resources:\n",
    "            row = {\n",
    "                'observation_id': obs.get('id', ''),\n",
    "                'patient_id': '',\n",
    "                'status': obs.get('status', ''),\n",
    "                'effective_date': obs.get('effectiveDateTime', ''),\n",
    "                'code': '',\n",
    "                'display': '',\n",
    "                'value': '',\n",
    "                'unit': ''\n",
    "            }\n",
    "            \n",
    "            # Extract patient reference\n",
    "            if 'subject' in obs and 'reference' in obs['subject']:\n",
    "                row['patient_id'] = obs['subject']['reference'].replace('Patient/', '')\n",
    "            \n",
    "            # Extract code information\n",
    "            if 'code' in obs and 'coding' in obs['code']:\n",
    "                coding = obs['code']['coding'][0]\n",
    "                row['code'] = coding.get('code', '')\n",
    "                row['display'] = coding.get('display', '')\n",
    "            \n",
    "            # Extract value\n",
    "            if 'valueQuantity' in obs:\n",
    "                value_qty = obs['valueQuantity']\n",
    "                row['value'] = value_qty.get('value', '')\n",
    "                row['unit'] = value_qty.get('unit', '')\n",
    "            elif 'valueString' in obs:\n",
    "                row['value'] = obs['valueString']\n",
    "            \n",
    "            obs_data.append(row)\n",
    "        \n",
    "        return pd.DataFrame(obs_data)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported resource type: {resource_type}\")\n",
    "\n",
    "# Transform resources to DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "for resource_type, resources in bundle_results['resources_by_type'].items():\n",
    "    if resource_type in ['Patient', 'Observation']:\n",
    "        df = transform_fhir_to_tabular(resources, resource_type)\n",
    "        dataframes[resource_type] = df\n",
    "        \n",
    "        print(f\"\\n{resource_type} DataFrame:\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        display(df)\n",
    "\n",
    "print(\"\\n✅ FHIR resources successfully converted to tabular format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Real-time Healthcare Data Pipeline\n",
    "\n",
    "Healthcare systems often need to process streaming data in real-time. Let's build a comprehensive real-time data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeHealthcareDataPipeline:\n",
    "    \"\"\"\n",
    "    Real-time healthcare data processing pipeline.\n",
    "    \n",
    "    This class implements a scalable pipeline for processing streaming\n",
    "    healthcare data with quality checks and alerting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"healthcare_pipeline.db\"):\n",
    "        \"\"\"Initialize the real-time pipeline.\"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.setup_database()\n",
    "        self.alert_thresholds = {\n",
    "            'heart_rate': {'min': 50, 'max': 120},\n",
    "            'blood_pressure_systolic': {'min': 90, 'max': 180},\n",
    "            'temperature': {'min': 96.0, 'max': 102.0},\n",
    "            'oxygen_saturation': {'min': 90, 'max': 100}\n",
    "        }\n",
    "        self.processed_count = 0\n",
    "        self.alert_count = 0\n",
    "    \n",
    "    def setup_database(self):\n",
    "        \"\"\"Setup SQLite database for pipeline storage.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create tables\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS patient_vitals (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                patient_id TEXT NOT NULL,\n",
    "                timestamp DATETIME NOT NULL,\n",
    "                vital_type TEXT NOT NULL,\n",
    "                value REAL NOT NULL,\n",
    "                unit TEXT,\n",
    "                alert_triggered BOOLEAN DEFAULT FALSE,\n",
    "                processed_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS alerts (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                patient_id TEXT NOT NULL,\n",
    "                alert_type TEXT NOT NULL,\n",
    "                message TEXT NOT NULL,\n",
    "                severity TEXT NOT NULL,\n",
    "                triggered_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                acknowledged BOOLEAN DEFAULT FALSE\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    async def process_vital_signs_stream(self, vital_signs_data: List[Dict[str, Any]]):\n",
    "        \"\"\"Process streaming vital signs data.\"\"\"\n",
    "        tasks = []\n",
    "        \n",
    "        for vital_data in vital_signs_data:\n",
    "            task = asyncio.create_task(self.process_single_vital(vital_data))\n",
    "            tasks.append(task)\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Log processing results\n",
    "        successful = sum(1 for r in results if not isinstance(r, Exception))\n",
    "        failed = len(results) - successful\n",
    "        \n",
    "        logger.info(f\"Processed {successful} vital signs, {failed} failed\")\n",
    "        self.processed_count += successful\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def process_single_vital(self, vital_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single vital sign measurement.\"\"\"\n",
    "        try:\n",
    "            # Validate required fields\n",
    "            required_fields = ['patient_id', 'vital_type', 'value', 'timestamp']\n",
    "            if not all(field in vital_data for field in required_fields):\n",
    "                raise ValueError(f\"Missing required fields: {required_fields}\")\n",
    "            \n",
    "            patient_id = vital_data['patient_id']\n",
    "            vital_type = vital_data['vital_type']\n",
    "            value = float(vital_data['value'])\n",
    "            timestamp = vital_data['timestamp']\n",
    "            unit = vital_data.get('unit', '')\n",
    "            \n",
    "            # Store in database\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute('''\n",
    "                INSERT INTO patient_vitals (patient_id, timestamp, vital_type, value, unit)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "            ''', (patient_id, timestamp, vital_type, value, unit))\n",
    "            \n",
    "            vital_id = cursor.lastrowid\n",
    "            \n",
    "            # Check for alerts\n",
    "            alert_triggered = await self.check_vital_alerts(patient_id, vital_type, value)\n",
    "            \n",
    "            if alert_triggered:\n",
    "                cursor.execute('''\n",
    "                    UPDATE patient_vitals SET alert_triggered = TRUE WHERE id = ?\n",
    "                ''', (vital_id,))\n",
    "                self.alert_count += 1\n",
    "            \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'vital_id': vital_id,\n",
    "                'alert_triggered': alert_triggered\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing vital sign: {e}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    async def check_vital_alerts(self, patient_id: str, vital_type: str, value: float) -> bool:\n",
    "        \"\"\"Check if vital sign triggers an alert.\"\"\"\n",
    "        if vital_type not in self.alert_thresholds:\n",
    "            return False\n",
    "        \n",
    "        thresholds = self.alert_thresholds[vital_type]\n",
    "        \n",
    "        if value < thresholds['min'] or value > thresholds['max']:\n",
    "            # Determine severity\n",
    "            if vital_type == 'heart_rate':\n",
    "                if value < 40 or value > 150:\n",
    "                    severity = 'CRITICAL'\n",
    "                else:\n",
    "                    severity = 'WARNING'\n",
    "            elif vital_type == 'blood_pressure_systolic':\n",
    "                if value < 70 or value > 200:\n",
    "                    severity = 'CRITICAL'\n",
    "                else:\n",
    "                    severity = 'WARNING'\n",
    "            else:\n",
    "                severity = 'WARNING'\n",
    "            \n",
    "            # Create alert\n",
    "            await self.create_alert(\n",
    "                patient_id=patient_id,\n",
    "                alert_type=f\"{vital_type}_abnormal\",\n",
    "                message=f\"{vital_type.replace('_', ' ').title()} value {value} is outside normal range\",\n",
    "                severity=severity\n",
    "            )\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    async def create_alert(self, patient_id: str, alert_type: str, message: str, severity: str):\n",
    "        \"\"\"Create a new alert in the database.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT INTO alerts (patient_id, alert_type, message, severity)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', (patient_id, alert_type, message, severity))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"🚨 ALERT [{severity}] for patient {patient_id}: {message}\")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = RealTimeHealthcareDataPipeline()\n",
    "print(\"✓ Real-time healthcare data pipeline initialized\")\n",
    "print(f\"✓ Alert thresholds configured for: {list(pipeline.alert_thresholds.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sample Streaming Data\n",
    "\n",
    "Let's generate some sample streaming vital signs data to test our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_vital_signs(n_patients: int = 10, hours: int = 2) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate sample vital signs data for testing.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    vital_signs = []\n",
    "    \n",
    "    patient_ids = [f'P{i:03d}' for i in range(1, n_patients + 1)]\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        for hour in range(hours):\n",
    "            timestamp = datetime.now() - timedelta(hours=hours-1-hour)\n",
    "            \n",
    "            # Generate heart rate (with some outliers)\n",
    "            base_hr = 70 + np.random.normal(0, 10)\n",
    "            if np.random.random() < 0.1:  # 10% chance of outlier\n",
    "                base_hr += np.random.choice([-40, 60])\n",
    "            \n",
    "            vital_signs.append({\n",
    "                'patient_id': patient_id,\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'vital_type': 'heart_rate',\n",
    "                'value': max(30, min(200, base_hr)),\n",
    "                'unit': 'bpm'\n",
    "            })\n",
    "            \n",
    "            # Generate blood pressure\n",
    "            systolic = 120 + np.random.normal(0, 15)\n",
    "            if np.random.random() < 0.05:  # 5% chance of hypertensive crisis\n",
    "                systolic += 80\n",
    "            \n",
    "            vital_signs.append({\n",
    "                'patient_id': patient_id,\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'vital_type': 'blood_pressure_systolic',\n",
    "                'value': max(70, min(250, systolic)),\n",
    "                'unit': 'mmHg'\n",
    "            })\n",
    "            \n",
    "            # Generate temperature\n",
    "            temp = 98.6 + np.random.normal(0, 1)\n",
    "            if np.random.random() < 0.08:  # 8% chance of fever\n",
    "                temp += np.random.uniform(3, 6)\n",
    "            \n",
    "            vital_signs.append({\n",
    "                'patient_id': patient_id,\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'vital_type': 'temperature',\n",
    "                'value': temp,\n",
    "                'unit': 'F'\n",
    "            })\n",
    "    \n",
    "    return vital_signs\n",
    "\n",
    "# Generate sample data\n",
    "sample_vitals = generate_sample_vital_signs(n_patients=5, hours=3)\n",
    "\n",
    "print(f\"✓ Generated {len(sample_vitals)} vital sign measurements\")\n",
    "print(f\"✓ Data covers {len(set(v['patient_id'] for v in sample_vitals))} patients\")\n",
    "print(f\"✓ Vital types: {set(v['vital_type'] for v in sample_vitals)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample vital signs data:\")\n",
    "sample_df = pd.DataFrame(sample_vitals[:10])\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Streaming Data\n",
    "\n",
    "Now let's process the streaming data through our real-time pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the streaming vital signs data\n",
    "print(\"Processing streaming vital signs data...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Process in batches to simulate real-time streaming\n",
    "batch_size = 10\n",
    "total_processed = 0\n",
    "total_alerts = 0\n",
    "\n",
    "for i in range(0, len(sample_vitals), batch_size):\n",
    "    batch = sample_vitals[i:i+batch_size]\n",
    "    \n",
    "    print(f\"\\nProcessing batch {i//batch_size + 1} ({len(batch)} records)...\")\n",
    "    \n",
    "    # Process batch asynchronously\n",
    "    results = await pipeline.process_vital_signs_stream(batch)\n",
    "    \n",
    "    # Count successful processing and alerts\n",
    "    batch_processed = sum(1 for r in results if isinstance(r, dict) and r.get('status') == 'success')\n",
    "    batch_alerts = sum(1 for r in results if isinstance(r, dict) and r.get('alert_triggered', False))\n",
    "    \n",
    "    total_processed += batch_processed\n",
    "    total_alerts += batch_alerts\n",
    "    \n",
    "    print(f\"  ✓ Processed: {batch_processed}/{len(batch)} records\")\n",
    "    print(f\"  🚨 Alerts triggered: {batch_alerts}\")\n",
    "\n",
    "print(f\"\\n📊 Final Processing Summary:\")\n",
    "print(f\"  Total records processed: {total_processed}\")\n",
    "print(f\"  Total alerts triggered: {total_alerts}\")\n",
    "print(f\"  Alert rate: {total_alerts/total_processed*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Analytics and Monitoring\n",
    "\n",
    "Let's analyze the pipeline performance and view the alerts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pipeline statistics\n",
    "def get_pipeline_statistics(db_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get comprehensive pipeline statistics.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Get vital signs statistics\n",
    "    vitals_df = pd.read_sql_query('''\n",
    "        SELECT \n",
    "            vital_type, \n",
    "            COUNT(*) as count, \n",
    "            AVG(value) as avg_value,\n",
    "            MIN(value) as min_value,\n",
    "            MAX(value) as max_value,\n",
    "            SUM(CASE WHEN alert_triggered THEN 1 ELSE 0 END) as alerts\n",
    "        FROM patient_vitals\n",
    "        GROUP BY vital_type\n",
    "    ''', conn)\n",
    "    \n",
    "    # Get alert statistics\n",
    "    alerts_df = pd.read_sql_query('''\n",
    "        SELECT \n",
    "            severity, \n",
    "            COUNT(*) as count,\n",
    "            alert_type\n",
    "        FROM alerts\n",
    "        GROUP BY severity, alert_type\n",
    "        ORDER BY severity, count DESC\n",
    "    ''', conn)\n",
    "    \n",
    "    # Get recent alerts\n",
    "    recent_alerts_df = pd.read_sql_query('''\n",
    "        SELECT \n",
    "            patient_id,\n",
    "            alert_type,\n",
    "            message,\n",
    "            severity,\n",
    "            triggered_at\n",
    "        FROM alerts\n",
    "        ORDER BY triggered_at DESC\n",
    "        LIMIT 10\n",
    "    ''', conn)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        'vitals_stats': vitals_df,\n",
    "        'alert_stats': alerts_df,\n",
    "        'recent_alerts': recent_alerts_df\n",
    "    }\n",
    "\n",
    "# Get and display statistics\n",
    "stats = get_pipeline_statistics(pipeline.db_path)\n",
    "\n",
    "print(\"📊 Pipeline Statistics\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"\\n1. Vital Signs Processing:\")\n",
    "display(stats['vitals_stats'])\n",
    "\n",
    "print(\"\\n2. Alert Statistics:\")\n",
    "if not stats['alert_stats'].empty:\n",
    "    display(stats['alert_stats'])\n",
    "else:\n",
    "    print(\"No alerts generated\")\n",
    "\n",
    "print(\"\\n3. Recent Alerts:\")\n",
    "if not stats['recent_alerts'].empty:\n",
    "    display(stats['recent_alerts'])\n",
    "else:\n",
    "    print(\"No recent alerts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Pipeline Performance\n",
    "\n",
    "Let's create some visualizations to better understand our pipeline performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Healthcare Data Pipeline Analytics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Vital signs distribution\n",
    "if not stats['vitals_stats'].empty:\n",
    "    ax1 = axes[0, 0]\n",
    "    stats['vitals_stats'].set_index('vital_type')['count'].plot(kind='bar', ax=ax1, color='skyblue')\n",
    "    ax1.set_title('Vital Signs Processing Volume')\n",
    "    ax1.set_ylabel('Number of Records')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Alert rate by vital type\n",
    "if not stats['vitals_stats'].empty:\n",
    "    ax2 = axes[0, 1]\n",
    "    vitals_with_alerts = stats['vitals_stats'][stats['vitals_stats']['alerts'] > 0]\n",
    "    if not vitals_with_alerts.empty:\n",
    "        alert_rates = (vitals_with_alerts['alerts'] / vitals_with_alerts['count'] * 100)\n",
    "        alert_rates.plot(kind='bar', ax=ax2, color='orange')\n",
    "        ax2.set_title('Alert Rate by Vital Type')\n",
    "        ax2.set_ylabel('Alert Rate (%)')\n",
    "        ax2.set_xticklabels(vitals_with_alerts['vital_type'], rotation=45)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No alerts generated', ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Alert Rate by Vital Type')\n",
    "\n",
    "# 3. Value distribution for heart rate\n",
    "conn = sqlite3.connect(pipeline.db_path)\n",
    "hr_data = pd.read_sql_query('''\n",
    "    SELECT value, alert_triggered \n",
    "    FROM patient_vitals \n",
    "    WHERE vital_type = \"heart_rate\"\n",
    "''', conn)\n",
    "conn.close()\n",
    "\n",
    "if not hr_data.empty:\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Plot normal vs alert values\n",
    "    normal_hr = hr_data[hr_data['alert_triggered'] == 0]['value']\n",
    "    alert_hr = hr_data[hr_data['alert_triggered'] == 1]['value']\n",
    "    \n",
    "    ax3.hist(normal_hr, bins=20, alpha=0.7, label='Normal', color='green')\n",
    "    if len(alert_hr) > 0:\n",
    "        ax3.hist(alert_hr, bins=20, alpha=0.7, label='Alert Triggered', color='red')\n",
    "    \n",
    "    # Add threshold lines\n",
    "    ax3.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='Min Threshold')\n",
    "    ax3.axvline(x=120, color='red', linestyle='--', alpha=0.5, label='Max Threshold')\n",
    "    \n",
    "    ax3.set_title('Heart Rate Distribution')\n",
    "    ax3.set_xlabel('Heart Rate (bpm)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.legend()\n",
    "\n",
    "# 4. Alert severity distribution\n",
    "if not stats['alert_stats'].empty:\n",
    "    ax4 = axes[1, 1]\n",
    "    severity_counts = stats['alert_stats'].groupby('severity')['count'].sum()\n",
    "    colors = {'WARNING': 'orange', 'CRITICAL': 'red'}\n",
    "    pie_colors = [colors.get(sev, 'gray') for sev in severity_counts.index]\n",
    "    \n",
    "    ax4.pie(severity_counts.values, labels=severity_counts.index, autopct='%1.1f%%', \n",
    "            colors=pie_colors, startangle=90)\n",
    "    ax4.set_title('Alert Severity Distribution')\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No alerts generated', ha='center', va='center', \n",
    "                    transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Alert Severity Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Pipeline analytics visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Quality Framework\n",
    "\n",
    "Data quality is crucial in healthcare AI. Let's implement a comprehensive data quality assessment framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthcareDataQualityFramework:\n",
    "    \"\"\"\n",
    "    Comprehensive data quality framework for healthcare data.\n",
    "    \n",
    "    This class implements data quality assessment, monitoring, and\n",
    "    improvement strategies specific to healthcare applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize data quality framework.\"\"\"\n",
    "        self.quality_rules = self._define_quality_rules()\n",
    "        self.quality_reports = []\n",
    "    \n",
    "    def _define_quality_rules(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Define healthcare-specific data quality rules.\"\"\"\n",
    "        return {\n",
    "            'completeness': {\n",
    "                'critical_fields': ['patient_id', 'timestamp', 'value'],\n",
    "                'threshold': 0.95  # 95% completeness required\n",
    "            },\n",
    "            'validity': {\n",
    "                'ranges': {\n",
    "                    'heart_rate': (20, 300),\n",
    "                    'blood_pressure_systolic': (50, 300),\n",
    "                    'blood_pressure_diastolic': (20, 200),\n",
    "                    'temperature': (90, 110),\n",
    "                    'weight': (1, 500),  # kg\n",
    "                    'height': (30, 250)  # cm\n",
    "                }\n",
    "            },\n",
    "            'consistency': {\n",
    "                'relationships': [\n",
    "                    ('systolic_bp', 'diastolic_bp', 'systolic >= diastolic')\n",
    "                ]\n",
    "            },\n",
    "            'timeliness': {\n",
    "                'max_delay_hours': 24\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def assess_data_quality(self, data: pd.DataFrame, data_type: str = 'general') -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive data quality assessment.\"\"\"\n",
    "        report = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'data_type': data_type,\n",
    "            'total_records': len(data),\n",
    "            'quality_scores': {},\n",
    "            'issues': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Completeness assessment\n",
    "        completeness_score, completeness_issues = self._assess_completeness(data)\n",
    "        report['quality_scores']['completeness'] = completeness_score\n",
    "        report['issues'].extend(completeness_issues)\n",
    "        \n",
    "        # Validity assessment\n",
    "        validity_score, validity_issues = self._assess_validity(data)\n",
    "        report['quality_scores']['validity'] = validity_score\n",
    "        report['issues'].extend(validity_issues)\n",
    "        \n",
    "        # Consistency assessment\n",
    "        consistency_score, consistency_issues = self._assess_consistency(data)\n",
    "        report['quality_scores']['consistency'] = consistency_score\n",
    "        report['issues'].extend(consistency_issues)\n",
    "        \n",
    "        # Timeliness assessment (if timestamp column exists)\n",
    "        if 'timestamp' in data.columns:\n",
    "            timeliness_score, timeliness_issues = self._assess_timeliness(data)\n",
    "            report['quality_scores']['timeliness'] = timeliness_score\n",
    "            report['issues'].extend(timeliness_issues)\n",
    "        \n",
    "        # Overall quality score\n",
    "        scores = list(report['quality_scores'].values())\n",
    "        report['overall_quality_score'] = np.mean(scores) if scores else 0\n",
    "        \n",
    "        # Generate recommendations\n",
    "        report['recommendations'] = self._generate_recommendations(report)\n",
    "        \n",
    "        self.quality_reports.append(report)\n",
    "        return report\n",
    "    \n",
    "    def _assess_completeness(self, data: pd.DataFrame) -> tuple[float, List[str]]:\n",
    "        \"\"\"Assess data completeness.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check critical fields\n",
    "        critical_fields = self.quality_rules['completeness']['critical_fields']\n",
    "        \n",
    "        for field in critical_fields:\n",
    "            if field in data.columns:\n",
    "                missing_pct = data[field].isnull().mean()\n",
    "                if missing_pct > 0:\n",
    "                    issues.append(f\"Critical field '{field}' has {missing_pct:.1%} missing values\")\n",
    "        \n",
    "        # Overall completeness score\n",
    "        total_missing = data.isnull().sum().sum()\n",
    "        total_values = data.size\n",
    "        completeness_score = 1 - (total_missing / total_values) if total_values > 0 else 0\n",
    "        \n",
    "        if completeness_score < self.quality_rules['completeness']['threshold']:\n",
    "            issues.append(f\"Overall completeness {completeness_score:.1%} below threshold\")\n",
    "        \n",
    "        return completeness_score, issues\n",
    "    \n",
    "    def _assess_validity(self, data: pd.DataFrame) -> tuple[float, List[str]]:\n",
    "        \"\"\"Assess data validity.\"\"\"\n",
    "        issues = []\n",
    "        validity_scores = []\n",
    "        \n",
    "        # Check value ranges for vital signs\n",
    "        if 'vital_type' in data.columns and 'value' in data.columns:\n",
    "            ranges = self.quality_rules['validity']['ranges']\n",
    "            \n",
    "            for vital_type, (min_val, max_val) in ranges.items():\n",
    "                vital_data = data[data['vital_type'] == vital_type]\n",
    "                if len(vital_data) > 0:\n",
    "                    out_of_range = ((vital_data['value'] < min_val) | (vital_data['value'] > max_val)).sum()\n",
    "                    total = len(vital_data)\n",
    "                    \n",
    "                    validity_rate = 1 - (out_of_range / total)\n",
    "                    validity_scores.append(validity_rate)\n",
    "                    \n",
    "                    if out_of_range > 0:\n",
    "                        issues.append(f\"Vital type '{vital_type}' has {out_of_range}/{total} values outside range [{min_val}, {max_val}]\")\n",
    "        \n",
    "        overall_validity = np.mean(validity_scores) if validity_scores else 1.0\n",
    "        return overall_validity, issues\n",
    "    \n",
    "    def _assess_consistency(self, data: pd.DataFrame) -> tuple[float, List[str]]:\n",
    "        \"\"\"Assess data consistency.\"\"\"\n",
    "        issues = []\n",
    "        consistency_scores = [1.0]  # Default to perfect consistency\n",
    "        \n",
    "        # Check for duplicate records\n",
    "        if 'patient_id' in data.columns and 'timestamp' in data.columns:\n",
    "            duplicates = data.duplicated(subset=['patient_id', 'timestamp', 'vital_type']).sum()\n",
    "            if duplicates > 0:\n",
    "                issues.append(f\"Found {duplicates} duplicate records\")\n",
    "                consistency_scores.append(1 - (duplicates / len(data)))\n",
    "        \n",
    "        # Check temporal consistency (timestamps should be in order)\n",
    "        if 'timestamp' in data.columns:\n",
    "            try:\n",
    "                timestamps = pd.to_datetime(data['timestamp'])\n",
    "                future_timestamps = (timestamps > datetime.now()).sum()\n",
    "                if future_timestamps > 0:\n",
    "                    issues.append(f\"Found {future_timestamps} future timestamps\")\n",
    "                    consistency_scores.append(1 - (future_timestamps / len(data)))\n",
    "            except:\n",
    "                issues.append(\"Invalid timestamp format detected\")\n",
    "                consistency_scores.append(0.5)\n",
    "        \n",
    "        overall_consistency = np.mean(consistency_scores)\n",
    "        return overall_consistency, issues\n",
    "    \n",
    "    def _assess_timeliness(self, data: pd.DataFrame) -> tuple[float, List[str]]:\n",
    "        \"\"\"Assess data timeliness.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        try:\n",
    "            timestamps = pd.to_datetime(data['timestamp'])\n",
    "            now = datetime.now()\n",
    "            \n",
    "            # Check for very old data\n",
    "            max_delay = timedelta(hours=self.quality_rules['timeliness']['max_delay_hours'])\n",
    "            old_data = (now - timestamps > max_delay).sum()\n",
    "            \n",
    "            timeliness_score = 1 - (old_data / len(data)) if len(data) > 0 else 1.0\n",
    "            \n",
    "            if old_data > 0:\n",
    "                issues.append(f\"{old_data} records are older than {max_delay}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            issues.append(f\"Error processing timestamps: {e}\")\n",
    "            timeliness_score = 0.0\n",
    "        \n",
    "        return timeliness_score, issues\n",
    "    \n",
    "    def _generate_recommendations(self, report: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate data quality improvement recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Completeness recommendations\n",
    "        if report['quality_scores'].get('completeness', 1.0) < 0.9:\n",
    "            recommendations.append(\"Implement data validation at point of entry to improve completeness\")\n",
    "            recommendations.append(\"Review data collection processes for critical fields\")\n",
    "        \n",
    "        # Validity recommendations\n",
    "        if report['quality_scores'].get('validity', 1.0) < 0.9:\n",
    "            recommendations.append(\"Add range validation checks for numeric fields\")\n",
    "            recommendations.append(\"Implement format validation for identifier fields\")\n",
    "        \n",
    "        # Consistency recommendations\n",
    "        if report['quality_scores'].get('consistency', 1.0) < 0.9:\n",
    "            recommendations.append(\"Add cross-field validation rules\")\n",
    "            recommendations.append(\"Implement automated consistency checks in data pipeline\")\n",
    "        \n",
    "        # Timeliness recommendations\n",
    "        if report['quality_scores'].get('timeliness', 1.0) < 0.9:\n",
    "            recommendations.append(\"Review data ingestion delays\")\n",
    "            recommendations.append(\"Implement real-time data quality monitoring\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Initialize quality framework\n",
    "quality_framework = HealthcareDataQualityFramework()\n",
    "print(\"✓ Healthcare Data Quality Framework initialized\")\n",
    "print(f\"✓ Quality dimensions: {list(quality_framework.quality_rules.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data Quality Assessment\n",
    "\n",
    "Let's test our data quality framework with some sample data that includes quality issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data with quality issues\n",
    "def create_test_data_with_issues() -> pd.DataFrame:\n",
    "    \"\"\"Create test data with various quality issues.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate base data\n",
    "    n_records = 100\n",
    "    data = []\n",
    "    \n",
    "    for i in range(n_records):\n",
    "        timestamp = datetime.now() - timedelta(hours=np.random.randint(0, 48))\n",
    "        \n",
    "        record = {\n",
    "            'patient_id': f'P{i%20:03d}',  # 20 unique patients\n",
    "            'timestamp': timestamp.isoformat(),\n",
    "            'vital_type': np.random.choice(['heart_rate', 'blood_pressure_systolic', 'temperature']),\n",
    "            'value': np.random.normal(100, 20),\n",
    "            'unit': 'bpm'\n",
    "        }\n",
    "        data.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Introduce quality issues\n",
    "    \n",
    "    # 1. Missing values (completeness issue)\n",
    "    missing_indices = np.random.choice(df.index, size=10, replace=False)\n",
    "    df.loc[missing_indices, 'value'] = np.nan\n",
    "    \n",
    "    # 2. Invalid values (validity issue)\n",
    "    invalid_indices = np.random.choice(df.index, size=5, replace=False)\n",
    "    df.loc[invalid_indices, 'value'] = -999  # Clearly invalid vital sign\n",
    "    \n",
    "    # 3. Out-of-range values (validity issue)\n",
    "    hr_indices = df[df['vital_type'] == 'heart_rate'].index\n",
    "    if len(hr_indices) > 0:\n",
    "        outlier_indices = np.random.choice(hr_indices, size=min(3, len(hr_indices)), replace=False)\n",
    "        df.loc[outlier_indices, 'value'] = 400  # Impossible heart rate\n",
    "    \n",
    "    # 4. Future timestamps (consistency issue)\n",
    "    future_indices = np.random.choice(df.index, size=2, replace=False)\n",
    "    future_time = datetime.now() + timedelta(days=1)\n",
    "    df.loc[future_indices, 'timestamp'] = future_time.isoformat()\n",
    "    \n",
    "    # 5. Duplicate records (consistency issue)\n",
    "    duplicate_record = df.iloc[0].copy()\n",
    "    df = pd.concat([df, pd.DataFrame([duplicate_record])], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create test data\n",
    "test_data = create_test_data_with_issues()\n",
    "\n",
    "print(f\"✓ Created test dataset with {len(test_data)} records\")\n",
    "print(f\"✓ Unique patients: {test_data['patient_id'].nunique()}\")\n",
    "print(f\"✓ Vital types: {test_data['vital_type'].unique()}\")\n",
    "print(f\"✓ Missing values: {test_data.isnull().sum().sum()}\")\n",
    "\n",
    "# Display sample of test data\n",
    "print(\"\\nSample test data:\")\n",
    "display(test_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Data Quality Assessment\n",
    "\n",
    "Now let's run our comprehensive data quality assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data quality assessment\n",
    "print(\"Running comprehensive data quality assessment...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quality_report = quality_framework.assess_data_quality(test_data, 'vital_signs')\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n📊 Data Quality Assessment Results\")\n",
    "print(f\"Assessment timestamp: {quality_report['timestamp']}\")\n",
    "print(f\"Dataset type: {quality_report['data_type']}\")\n",
    "print(f\"Total records: {quality_report['total_records']:,}\")\n",
    "\n",
    "print(f\"\\n🎯 Overall Quality Score: {quality_report['overall_quality_score']:.1%}\")\n",
    "\n",
    "print(f\"\\n📈 Quality Dimension Scores:\")\n",
    "for dimension, score in quality_report['quality_scores'].items():\n",
    "    status = \"✅\" if score >= 0.9 else \"⚠️\" if score >= 0.7 else \"❌\"\n",
    "    print(f\"  {status} {dimension.title()}: {score:.1%}\")\n",
    "\n",
    "print(f\"\\n⚠️ Issues Identified ({len(quality_report['issues'])}):\") \n",
    "for i, issue in enumerate(quality_report['issues'], 1):\n",
    "    print(f\"  {i}. {issue}\")\n",
    "\n",
    "print(f\"\\n💡 Recommendations ({len(quality_report['recommendations'])}):\") \n",
    "for i, rec in enumerate(quality_report['recommendations'], 1):\n",
    "    print(f\"  {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Visualization\n",
    "\n",
    "Let's create visualizations to better understand the data quality issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data quality visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Data Quality Assessment Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Quality scores radar chart (simplified as bar chart)\n",
    "ax1 = axes[0, 0]\n",
    "dimensions = list(quality_report['quality_scores'].keys())\n",
    "scores = list(quality_report['quality_scores'].values())\n",
    "colors = ['green' if s >= 0.9 else 'orange' if s >= 0.7 else 'red' for s in scores]\n",
    "\n",
    "bars = ax1.bar(dimensions, scores, color=colors, alpha=0.7)\n",
    "ax1.set_title('Quality Dimension Scores')\n",
    "ax1.set_ylabel('Quality Score')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add score labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.1%}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Missing values heatmap\n",
    "ax2 = axes[0, 1]\n",
    "missing_data = test_data.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    missing_pct = (missing_data / len(test_data) * 100)\n",
    "    bars = ax2.bar(missing_pct.index, missing_pct.values, color='red', alpha=0.7)\n",
    "    ax2.set_title('Missing Values by Column')\n",
    "    ax2.set_ylabel('Missing Percentage (%)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, pct in zip(bars, missing_pct.values):\n",
    "        if pct > 0:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                     f'{pct:.1f}%', ha='center', va='bottom')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No missing values', ha='center', va='center', transform=ax2.transAxes)\n",
    "    ax2.set_title('Missing Values by Column')\n",
    "\n",
    "# 3. Value distribution with outliers\n",
    "ax3 = axes[1, 0]\n",
    "valid_values = test_data['value'].dropna()\n",
    "valid_values = valid_values[valid_values > 0]  # Remove clearly invalid values\n",
    "\n",
    "if len(valid_values) > 0:\n",
    "    ax3.hist(valid_values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax3.set_title('Value Distribution (Valid Values Only)')\n",
    "    ax3.set_xlabel('Value')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = valid_values.mean()\n",
    "    std_val = valid_values.std()\n",
    "    ax3.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.1f}')\n",
    "    ax3.axvline(mean_val + 2*std_val, color='orange', linestyle='--', alpha=0.7, label='±2σ')\n",
    "    ax3.axvline(mean_val - 2*std_val, color='orange', linestyle='--', alpha=0.7)\n",
    "    ax3.legend()\n",
    "\n",
    "# 4. Quality trend (if multiple reports available)\n",
    "ax4 = axes[1, 1]\n",
    "if len(quality_framework.quality_reports) > 1:\n",
    "    # Plot quality trends over time\n",
    "    timestamps = [report['timestamp'] for report in quality_framework.quality_reports]\n",
    "    overall_scores = [report['overall_quality_score'] for report in quality_framework.quality_reports]\n",
    "    \n",
    "    ax4.plot(range(len(timestamps)), overall_scores, marker='o', linewidth=2, markersize=6)\n",
    "    ax4.set_title('Quality Score Trend')\n",
    "    ax4.set_xlabel('Assessment Number')\n",
    "    ax4.set_ylabel('Overall Quality Score')\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Show current quality score as gauge\n",
    "    score = quality_report['overall_quality_score']\n",
    "    color = 'green' if score >= 0.9 else 'orange' if score >= 0.7 else 'red'\n",
    "    \n",
    "    # Create a simple gauge visualization\n",
    "    theta = np.linspace(0, np.pi, 100)\n",
    "    r = np.ones_like(theta)\n",
    "    \n",
    "    ax4.plot(theta, r, 'k-', linewidth=3)\n",
    "    \n",
    "    # Add score indicator\n",
    "    score_theta = score * np.pi\n",
    "    ax4.plot([score_theta, score_theta], [0, 1], color=color, linewidth=4)\n",
    "    ax4.fill_between([0, score_theta], [0, 0], [1, 1], alpha=0.3, color=color)\n",
    "    \n",
    "    ax4.set_xlim(0, np.pi)\n",
    "    ax4.set_ylim(0, 1.2)\n",
    "    ax4.set_title(f'Overall Quality Score\\n{score:.1%}')\n",
    "    ax4.set_xticks([0, np.pi/2, np.pi])\n",
    "    ax4.set_xticklabels(['0%', '50%', '100%'])\n",
    "    ax4.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Data quality visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this comprehensive tutorial, we've covered the essential aspects of healthcare data engineering:\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **FHIR Processing**: Built a complete FHIR resource processor that can validate, transform, and convert healthcare data to tabular format for machine learning applications.\n",
    "\n",
    "2. **Real-time Data Pipeline**: Implemented a scalable real-time pipeline for processing streaming healthcare data with automated alerting and quality monitoring.\n",
    "\n",
    "3. **Data Quality Framework**: Created a comprehensive data quality assessment system that evaluates completeness, validity, consistency, and timeliness of healthcare data.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Healthcare data requires specialized handling** due to privacy, regulatory, and quality requirements\n",
    "- **Real-time processing is crucial** for clinical decision support and patient safety\n",
    "- **Data quality assessment must be continuous** and automated in healthcare environments\n",
    "- **FHIR standardization** enables interoperability but requires careful validation and transformation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Extend the FHIR processor** to handle additional resource types (Medication, Procedure, etc.)\n",
    "2. **Implement advanced alerting** with machine learning-based anomaly detection\n",
    "3. **Add privacy protection** with differential privacy and federated learning capabilities\n",
    "4. **Scale the pipeline** using cloud services and container orchestration\n",
    "5. **Integrate with EHR systems** for real-world deployment\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [FHIR Specification](https://www.hl7.org/fhir/)\n",
    "- [Healthcare Data Quality Guidelines](https://www.himss.org/resources/healthcare-data-quality)\n",
    "- [Real-time Healthcare Analytics](https://www.healthcatalyst.com/real-time-analytics/)\n",
    "\n",
    "Continue to **Chapter 4: Structured Machine Learning** to learn how to build predictive models using the clean, processed healthcare data from this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "print(\"🧹 Cleaning up resources...\")\n",
    "\n",
    "# Close database connections\n",
    "try:\n",
    "    if 'pipeline' in locals():\n",
    "        # The pipeline database will be cleaned up automatically\n",
    "        print(\"✓ Pipeline database connections closed\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"✅ Chapter 3 tutorial completed successfully!\")\n",
    "print(\"📚 Ready to proceed to Chapter 4: Structured Machine Learning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python\",\n",
   "name": "python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}
